{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Based on code from the following link:\r\n",
    "# https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install simpletransformers==0.34.4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import keras\r\n",
    "# keras.__version__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import tensorflow\r\n",
    "# tensorflow.__version__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import torch\r\n",
    "# torch.__version__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !jupyter --version"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install scikit-learn==0.23.1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install pandas==1.0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip show simpletransformers "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import sklearn\r\n",
    "# sklearn.__version__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\r\n",
    "#import ast"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Miniconda\\envs\\py38tfpt\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Miniconda\\envs\\py38tfpt\\lib\\site-packages\\torchaudio\\backend\\utils.py:88: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#from simpletransformers.experimental.classification import MultiLabelClassificationModel"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Miniconda\\envs\\py38tfpt\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Miniconda\\envs\\py38tfpt\\lib\\site-packages\\torchaudio\\backend\\utils.py:88: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv('train21Aug27.tsv', sep='\\t')\r\n",
    "#df = pd.read_csv('../Mycode/mimic3_preprocess/train.tsv', sep='\\t')\r\n",
    "df= df.sample(frac=0.2, random_state=57)\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                     text  \\\n",
       "252207  Newborn Med Attending\\n\\nAsked by Dr [**Last N...   \n",
       "185389  [**2125-2-13**] 1:07 AM\\n CT HEAD W/O CONTRAST...   \n",
       "299280  [**2145-4-5**] 11:34 AM\\n CHEST (PORTABLE AP) ...   \n",
       "62385   TITLE: Respiratory Care:\\n   Patient extubated...   \n",
       "73962   Neonatology Attending\\nDOL#32, CGA 30 [**1-9**...   \n",
       "\n",
       "                                                   labels  \n",
       "252207  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "185389  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "299280  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "62385   [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "73962   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>252207</th>\n",
       "      <td>Newborn Med Attending\\n\\nAsked by Dr [**Last N...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185389</th>\n",
       "      <td>[**2125-2-13**] 1:07 AM\\n CT HEAD W/O CONTRAST...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299280</th>\n",
       "      <td>[**2145-4-5**] 11:34 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62385</th>\n",
       "      <td>TITLE: Respiratory Care:\\n   Patient extubated...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73962</th>\n",
       "      <td>Neonatology Attending\\nDOL#32, CGA 30 [**1-9**...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.labels = df.labels.apply(eval)\r\n",
    "df.labels.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(86494,)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(86494, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#ab= df['labels'][0:1]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = MultiLabelClassificationModel('bert', \"emilyalsentzer/Bio_ClinicalBERT\", num_labels=101, \r\n",
    "                                      args={'train_batch_size':8,\r\n",
    "                                            'gradient_accumulation_steps':16,\r\n",
    "                                            'learning_rate': 5e-5,\r\n",
    "                                            'num_train_epochs': 2,\r\n",
    "                                            'max_seq_length': 250,\r\n",
    "                                            'reprocess_input_data': True,\r\n",
    "                                            'overwrite_output_dir': True,\r\n",
    "                                            'save_optimizer_and_scheduler': True})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMultiLabelSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model.train_model(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 173/86494 [01:49<15:07:22,  1.59it/s]\n",
      "Epochs 0/2. Running Loss:    0.2371: 100%|██████████| 10812/10812 [34:54<00:00,  5.16it/s]\n",
      "C:\\Miniconda\\envs\\py38tfpt\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "Epochs 1/2. Running Loss:    0.1939: 100%|██████████| 10812/10812 [33:40<00:00,  5.35it/s]\n",
      "Epoch 2 of 2: 100%|██████████| 2/2 [1:08:42<00:00, 2061.18s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1350, 0.21466032622244072)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sliding Window Experimental Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "import pandas as pd\r\n",
    "#from simpletransformers.classification import MultiLabelClassificationModel\r\n",
    "import ast\r\n",
    "#from simpletransformers.experimental.classification import MultiLabelClassificationModel\r\n",
    "\r\n",
    "df = pd.read_csv('./train.tsv', sep='\\t')\r\n",
    "df.labels = df.labels.apply(eval)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model = MultiLabelClassificationModel('bert', \"emilyalsentzer/Bio_ClinicalBERT\", num_labels=101, \r\n",
    "                                      args={'train_batch_size':4,\r\n",
    "                                            'sliding_window': True,\r\n",
    "                                            'stride': 0.8,\r\n",
    "                                            'gradient_accumulation_steps':16,\r\n",
    "                                            'learning_rate': 5e-5,\r\n",
    "                                            'num_train_epochs': 2,\r\n",
    "                                            'max_seq_length': 150,\r\n",
    "                                            'reprocess_input_data': True,\r\n",
    "                                            'overwrite_output_dir': True,\r\n",
    "                                            'no_cache': True,\r\n",
    "                                            'multiprocessing_chunksize': 410,\r\n",
    "                                           #'tensorboard_dir':'./outputs/',\r\n",
    "                                           #'wandb_project': False,\r\n",
    "                                           'save_model_every_epoch': True,\r\n",
    "                                           'no_save': False,\r\n",
    "                                           'save_optimizer_and_scheduler': True})"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model.train_model(df)"
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\r\n",
    "import ast\r\n",
    "#from simpletransformers.experimental.classification import MultiLabelClassificationModel\r\n",
    "import sklearn\r\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Miniconda\\envs\\py38tfpt\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Miniconda\\envs\\py38tfpt\\lib\\site-packages\\torchaudio\\backend\\utils.py:88: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model = MultiLabelClassificationModel('bert', './outputs/checkpoint-386-epoch-2', num_labels=101,\r\n",
    "                                      args={'train_batch_size':4,\r\n",
    "                                            'sliding_window': True,\r\n",
    "                                            'stride': 0.8,\r\n",
    "                                            'gradient_accumulation_steps':16,\r\n",
    "                                            'learning_rate': 3e-5,\r\n",
    "                                            'num_train_epochs': 3,\r\n",
    "                                            'max_seq_length': 512,\r\n",
    "                                            'reprocess_input_data': True,\r\n",
    "                                            'overwrite_output_dir': True,\r\n",
    "                                            'no_cache': True,\r\n",
    "                                            'multiprocessing_chunksize': 410,\r\n",
    "                                           #'tensorboard_dir':'./outputs/',\r\n",
    "                                           #'wandb_project': False,\r\n",
    "                                           'save_model_every_epoch': True,\r\n",
    "                                           'no_save': False,\r\n",
    "                                           'save_optimizer_and_scheduler': True})"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = MultiLabelClassificationModel('bert', \"./outputs/checkpoint-1350-epoch-2\", num_labels=101, \r\n",
    "                                      args={'train_batch_size':8,\r\n",
    "                                            'gradient_accumulation_steps':16,\r\n",
    "                                            'learning_rate': 5e-5,\r\n",
    "                                            'num_train_epochs': 2,\r\n",
    "                                            'max_seq_length': 250,\r\n",
    "                                            'reprocess_input_data': True,\r\n",
    "                                            'overwrite_output_dir': True,\r\n",
    "                                            'save_optimizer_and_scheduler': True})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\r\n",
    "eval_df = pd.read_csv('test21Aug27.tsv', sep='\\t')\r\n",
    "eval_df= eval_df.sample(frac=0.2, random_state=57, ignore_index=True)\r\n",
    "eval_df.labels = eval_df.labels.apply(eval)\r\n",
    "print(eval_df.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10812, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print(eval_df.tail())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                   text  \\\n",
      "5401  TITLE:\\n   Chief Complaint:\\n   Pulmonary\\n   ...   \n",
      "5402  88yo woman w hx of HTN, hypothyroidism, hyperl...   \n",
      "5403  Respiratory Care\\nBaby continues on prong CPAP...   \n",
      "5404  Neonatology Attending Note\\n\\nInfant well-know...   \n",
      "5405  Neonatology NP Note\\nPLease refer to attending...   \n",
      "\n",
      "                                                 labels  \n",
      "5401  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "5402  [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "5403  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "5404  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "5405  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\r\n",
    "#model.get_named_parameters()\r\n",
    "predictions, raw_outputs, wrong_pred = model.eval_model(eval_df)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 22/10812 [00:28<3:54:20,  1.30s/it]\n",
      "Running Evaluation: 100%|██████████| 1352/1352 [01:38<00:00, 13.77it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print(raw_outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.06524658 0.14440918 0.10821533 ... 0.01751709 0.07922363 0.05633545]\n",
      " [0.08282471 0.19348145 0.12744141 ... 0.01757812 0.08959961 0.06994629]\n",
      " [0.02700806 0.02975464 0.04647827 ... 0.13708496 0.0411377  0.03442383]\n",
      " ...\n",
      " [0.06695557 0.15319824 0.10931396 ... 0.016922   0.07922363 0.05697632]\n",
      " [0.03044128 0.03216553 0.05072021 ... 0.15539551 0.0440979  0.03759766]\n",
      " [0.1427002  0.27563477 0.18457031 ... 0.02423096 0.12023926 0.09967041]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\r\n",
    "y_true = np.array(eval_df['labels'].tolist())\r\n",
    "y_true[y_true > 1] = 1\r\n",
    "#print(y_true)\r\n",
    "print(f\"y_true: {y_true.shape}\")\r\n",
    "\r\n",
    "#m_outputs = raw_outputs[0:10]\r\n",
    "for cut_off in np.linspace(0.1,1,9,endpoint=False):  \r\n",
    "    y_pred = np.empty((0,101))\r\n",
    "    for i in range(len(raw_outputs)):\r\n",
    "        m_output = np.where (raw_outputs[i]>cut_off, 1,0) \r\n",
    "        y_pred = np.vstack([y_pred,m_output])\r\n",
    "    \r\n",
    "    print(f\"y_true shape: {y_true.shape}  y_pred Shape: {y_pred.shape}\")\r\n",
    "    print(\"\\n\\nCalculated F-scores for cutoff:\", cut_off)\r\n",
    "    print(\"\\nMacro:\", sklearn.metrics.f1_score(y_true, y_pred, average='macro'))\r\n",
    "    print(\"Micro:\", sklearn.metrics.f1_score(y_true, y_pred, average='micro'))\r\n",
    "    print(\"Weighted:\", sklearn.metrics.f1_score(y_true, y_pred, average='weighted'))\r\n",
    "    print(\"Samples:\",sklearn.metrics.f1_score(y_true, y_pred, average='samples', zero_division=1))\r\n",
    "    #print(\"None:\\n\",sklearn.metrics.f1_score(y_true, y_pred, average=None))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y_true: (10812, 101)\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.1\n",
      "\n",
      "Macro: 0.23734861068623364\n",
      "Micro: 0.3771816127538286\n",
      "Weighted: 0.4084807902050138\n",
      "Samples: 0.3736207804009601\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.2\n",
      "\n",
      "Macro: 0.1696405054931424\n",
      "Micro: 0.4614686614730878\n",
      "Weighted: 0.3707221727595345\n",
      "Samples: 0.45041401813233684\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.30000000000000004\n",
      "\n",
      "Macro: 0.12627480205396535\n",
      "Micro: 0.4597470248403319\n",
      "Weighted: 0.33145817307829417\n",
      "Samples: 0.4555119685897118\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.4\n",
      "\n",
      "Macro: 0.09479958915027803\n",
      "Micro: 0.4189769017203618\n",
      "Weighted: 0.29511919483942306\n",
      "Samples: 0.42518574846850227\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.5\n",
      "\n",
      "Macro: 0.06817706971746926\n",
      "Micro: 0.36331962166747234\n",
      "Weighted: 0.24520634475189745\n",
      "Samples: 0.38155028632143345\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.6\n",
      "\n",
      "Macro: 0.03306957044567068\n",
      "Micro: 0.28853648029973655\n",
      "Weighted: 0.17950930258618222\n",
      "Samples: 0.3293537241548544\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.7000000000000001\n",
      "\n",
      "Macro: 0.024291745628869668\n",
      "Micro: 0.2650548145721803\n",
      "Weighted: 0.15616759013062628\n",
      "Samples: 0.3116224113852861\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.8\n",
      "\n",
      "Macro: 0.019125071585838063\n",
      "Micro: 0.24634131152305233\n",
      "Weighted: 0.1456082008556745\n",
      "Samples: 0.2967014866333087\n",
      "y_true shape: (10812, 101)  y_pred Shape: (10812, 101)\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.9\n",
      "\n",
      "Macro: 0.009844349859849945\n",
      "Micro: 0.2249723756906077\n",
      "Weighted: 0.12620203318781592\n",
      "Samples: 0.2803586391021616\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare to 2 Epoch Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model = MultiLabelClassificationModel('bert', \"./outputs/checkpoint-63954-epoch-2\", num_labels=124,\r\n",
    "                                      args={'train_batch_size':4,\r\n",
    "                                            'sliding_window': True,\r\n",
    "                                            'stride': 0.8,\r\n",
    "                                            'gradient_accumulation_steps':16,\r\n",
    "                                            'learning_rate': 3e-5,\r\n",
    "                                            'num_train_epochs': 3,\r\n",
    "                                            'max_seq_length': 512,\r\n",
    "                                            'reprocess_input_data': True,\r\n",
    "                                            'overwrite_output_dir': True,\r\n",
    "                                            'no_cache': True,\r\n",
    "                                            'multiprocessing_chunksize': 410,\r\n",
    "                                           'tensorboard_dir':'./outputs/',\r\n",
    "                                           'wandb_project': False,\r\n",
    "                                           'save_model_every_epoch': True,\r\n",
    "                                           'no_save': False,\r\n",
    "                                           'save_optimizer_and_scheduler': True})"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "predictions, raw_outputs = model.predict(eval_df['text'])"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "y_true = np.array(eval_df['labels'].tolist())\r\n",
    "y_true[y_true > 1] = 1\r\n",
    "\r\n",
    "for cut_off in np.linspace(0.1,1,9,endpoint=False):  \r\n",
    "\r\n",
    "    y_pred = np.empty((0,124))\r\n",
    "\r\n",
    "    for i in range(len(raw_outputs)):\r\n",
    "        temp = np.max(raw_outputs[i], axis=0)\r\n",
    "        temp[temp>cut_off] = 1\r\n",
    "        temp[temp<=cut_off] = 0\r\n",
    "        y_pred = np.vstack([y_pred,temp])\r\n",
    "    \r\n",
    "    print(\"\\n\\nCalculated F-scores for cutoff:\", cut_off)\r\n",
    "    print(\"\\nMacro:\", sklearn.metrics.f1_score(y_true, y_pred, average='macro'))\r\n",
    "    print(\"Micro:\", sklearn.metrics.f1_score(y_true, y_pred, average='micro'))\r\n",
    "    print(\"Weighted:\", sklearn.metrics.f1_score(y_true, y_pred, average='weighted'))\r\n",
    "    print(\"Samples:\",sklearn.metrics.f1_score(y_true, y_pred, average='samples', zero_division=1))\r\n",
    "    print(\"None:\\n\",sklearn.metrics.f1_score(y_true, y_pred, average=None))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.1\n",
      "\n",
      "Macro: 0.1732386101703209\n",
      "Micro: 0.45128354725787634\n",
      "Weighted: 0.5125896828306503\n",
      "Samples: 0.4456942534521216\n",
      "None:\n",
      " [0.84465825 0.83954097 0.81695389 0.65557316 0.60609388 0.42431762\n",
      " 0.28475034 0.75722294 0.54137931 0.48145933 0.44047619 0.84465825\n",
      " 0.60435835 0.52771392 0.59107291 0.         0.20014144 0.31687106\n",
      " 0.2479564  0.6491018  0.5382193  0.60609388 0.83954097 0.\n",
      " 0.13107511 0.         0.17410072 0.31193881 0.         0.\n",
      " 0.29090909 0.         0.         0.         0.1641953  0.\n",
      " 0.         0.         0.         0.         0.         0.14812112\n",
      " 0.         0.18129703 0.11581292 0.         0.         0.\n",
      " 0.         0.         0.         0.57175014 0.         0.\n",
      " 0.         0.         0.         0.         0.36732068 0.\n",
      " 0.         0.         0.         0.         0.38446849 0.\n",
      " 0.         0.         0.41176471 0.         0.         0.\n",
      " 0.         0.         0.         0.16389892 0.         0.\n",
      " 0.13102687 0.         0.         0.         0.26580116 0.2206655\n",
      " 0.         0.         0.07581501 0.31697613 0.         0.\n",
      " 0.2069209  0.         0.         0.         0.21800948 0.35568513\n",
      " 0.         0.09459459 0.         0.         0.         0.14134017\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.63297232 0.         0.         0.         0.\n",
      " 0.64994026 0.41573034 0.42777778 0.         0.         0.\n",
      " 0.         0.28006088 0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.2\n",
      "\n",
      "Macro: 0.14858551771669543\n",
      "Micro: 0.5417484419653618\n",
      "Weighted: 0.49558936332876713\n",
      "Samples: 0.5295398680403876\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.65557316 0.60609388 0.42431762\n",
      " 0.         0.75722294 0.54137931 0.48145933 0.44047619 0.84465825\n",
      " 0.60435835 0.52771392 0.59107291 0.         0.20014144 0.31687106\n",
      " 0.2479564  0.6491018  0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.57175014 0.         0.\n",
      " 0.         0.         0.         0.         0.36732068 0.\n",
      " 0.         0.         0.         0.         0.38446849 0.\n",
      " 0.         0.         0.41176471 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.26580116 0.\n",
      " 0.         0.         0.         0.31697613 0.         0.\n",
      " 0.         0.         0.         0.         0.21800948 0.35568513\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.63297232 0.         0.         0.         0.\n",
      " 0.64994026 0.41573034 0.42777778 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.30000000000000004\n",
      "\n",
      "Macro: 0.13626312179388955\n",
      "Micro: 0.5631108735491753\n",
      "Weighted: 0.4827844140871503\n",
      "Samples: 0.5519699240807215\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.65557316 0.60609388 0.42431762\n",
      " 0.         0.75722294 0.54137931 0.48145933 0.48150358 0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.         0.\n",
      " 0.2479564  0.6491018  0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.57175014 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.38446849 0.\n",
      " 0.         0.         0.41176471 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.26580116 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.21800948 0.35568513\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.63297232 0.         0.         0.         0.\n",
      " 0.64994026 0.         0.42777778 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.4\n",
      "\n",
      "Macro: 0.1196376195571772\n",
      "Micro: 0.567281929145579\n",
      "Weighted: 0.4535628792918775\n",
      "Samples: 0.5560244449236706\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.65557316 0.60609388 0.42431762\n",
      " 0.         0.75722294 0.54137931 0.48145933 0.48150358 0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.         0.\n",
      " 0.2479564  0.6491018  0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.21800948 0.35568513\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.63297232 0.         0.         0.         0.\n",
      " 0.64994026 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.5\n",
      "\n",
      "Macro: 0.10987214122358395\n",
      "Micro: 0.5649459349409582\n",
      "Weighted: 0.434407410449084\n",
      "Samples: 0.5551448034888926\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.65557316 0.60609388 0.42431762\n",
      " 0.         0.75722294 0.54137931 0.         0.         0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.         0.\n",
      " 0.         0.6491018  0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.21800948 0.35568513\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.63297232 0.         0.         0.         0.\n",
      " 0.64994026 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.6\n",
      "\n",
      "Macro: 0.08945816089651208\n",
      "Micro: 0.5531821076870316\n",
      "Weighted: 0.4035990325907193\n",
      "Samples: 0.5396255442023211\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.65557316 0.         0.\n",
      " 0.         0.75722294 0.54137931 0.         0.         0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.         0.\n",
      " 0.         0.6491018  0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.35568513\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.7000000000000001\n",
      "\n",
      "Macro: 0.07082170234603283\n",
      "Micro: 0.5156101381972807\n",
      "Weighted: 0.3468217060142614\n",
      "Samples: 0.5122318229932645\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.         0.         0.\n",
      " 0.         0.75722294 0.         0.         0.         0.84465825\n",
      " 0.60435835 0.         0.59107291 0.         0.         0.\n",
      " 0.         0.6491018  0.         0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.35568513\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.8\n",
      "\n",
      "Macro: 0.05286054999368093\n",
      "Micro: 0.4370573236377373\n",
      "Weighted: 0.27499699524974497\n",
      "Samples: 0.4428133241896826\n",
      "None:\n",
      " [0.90419806 0.39109098 0.         0.         0.         0.\n",
      " 0.         0.75722294 0.         0.         0.         0.84465825\n",
      " 0.60435835 0.         0.59107291 0.         0.         0.\n",
      " 0.         0.6491018  0.         0.         0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.9\n",
      "\n",
      "Macro: 0.040801913088749586\n",
      "Micro: 0.3666906154846214\n",
      "Weighted: 0.21588697049729288\n",
      "Samples: 0.38050082904935856\n",
      "None:\n",
      " [0.         0.39109098 0.         0.         0.         0.\n",
      " 0.         0.75722294 0.         0.         0.         0.84465825\n",
      " 0.60435835 0.         0.         0.         0.         0.\n",
      " 0.         0.6491018  0.         0.         0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare to 1 Epoch Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model = MultiLabelClassificationModel('bert', \"./outputs/checkpoint-31977-epoch-1\", num_labels=124,\r\n",
    "                                      args={'train_batch_size':4,\r\n",
    "                                            'sliding_window': True,\r\n",
    "                                            'stride': 0.8,\r\n",
    "                                            'gradient_accumulation_steps':16,\r\n",
    "                                            'learning_rate': 3e-5,\r\n",
    "                                            'num_train_epochs': 3,\r\n",
    "                                            'max_seq_length': 512,\r\n",
    "                                            'reprocess_input_data': True,\r\n",
    "                                            'overwrite_output_dir': True,\r\n",
    "                                            'no_cache': True,\r\n",
    "                                            'multiprocessing_chunksize': 410,\r\n",
    "                                           'tensorboard_dir':'./outputs/',\r\n",
    "                                           'wandb_project': False,\r\n",
    "                                           'save_model_every_epoch': True,\r\n",
    "                                           'no_save': False,\r\n",
    "                                           'save_optimizer_and_scheduler': True})"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "predictions, raw_outputs = model.predict(eval_df['text'])"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "y_true = np.array(eval_df['labels'].tolist())\r\n",
    "y_true[y_true > 1] = 1\r\n",
    "\r\n",
    "for cut_off in np.linspace(0.1,1,9,endpoint=False):  \r\n",
    "\r\n",
    "    y_pred = np.empty((0,124))\r\n",
    "\r\n",
    "    for i in range(len(raw_outputs)):\r\n",
    "        temp = np.max(raw_outputs[i], axis=0)\r\n",
    "        temp[temp>cut_off] = 1\r\n",
    "        temp[temp<=cut_off] = 0\r\n",
    "        y_pred = np.vstack([y_pred,temp])\r\n",
    "    \r\n",
    "    print(\"\\n\\nCalculated F-scores for cutoff:\", cut_off)\r\n",
    "    print(\"\\nMacro:\", sklearn.metrics.f1_score(y_true, y_pred, average='macro'))\r\n",
    "    print(\"Micro:\", sklearn.metrics.f1_score(y_true, y_pred, average='micro'))\r\n",
    "    print(\"Weighted:\", sklearn.metrics.f1_score(y_true, y_pred, average='weighted'))\r\n",
    "    print(\"Samples:\",sklearn.metrics.f1_score(y_true, y_pred, average='samples', zero_division=1))\r\n",
    "    print(\"None:\\n\",sklearn.metrics.f1_score(y_true, y_pred, average=None))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.1\n",
      "\n",
      "Macro: 0.1757971450486654\n",
      "Micro: 0.446947332794314\n",
      "Weighted: 0.5164590768777718\n",
      "Samples: 0.44343468159506666\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.6550449  0.60609388 0.42431762\n",
      " 0.28475034 0.75722294 0.54137931 0.48145933 0.44047619 0.84465825\n",
      " 0.60435835 0.52771392 0.59107291 0.         0.20014144 0.31687106\n",
      " 0.2553802  0.67919799 0.5382193  0.60609388 0.83954097 0.\n",
      " 0.13107511 0.         0.17410072 0.31193881 0.         0.\n",
      " 0.29090909 0.         0.08308157 0.         0.1641953  0.\n",
      " 0.         0.         0.         0.         0.         0.14812112\n",
      " 0.         0.18129703 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.57175014 0.         0.\n",
      " 0.         0.         0.         0.         0.36732068 0.\n",
      " 0.         0.         0.         0.         0.38446849 0.\n",
      " 0.         0.         0.41176471 0.         0.         0.\n",
      " 0.         0.         0.         0.16389892 0.         0.\n",
      " 0.13102687 0.09741476 0.         0.         0.26580116 0.2206655\n",
      " 0.         0.         0.07581501 0.31697613 0.         0.\n",
      " 0.2069209  0.         0.         0.         0.23154362 0.37596302\n",
      " 0.         0.09459459 0.         0.         0.         0.14134017\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.66246851 0.         0.         0.         0.\n",
      " 0.68       0.43851852 0.45095168 0.         0.         0.\n",
      " 0.         0.29677419 0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.2\n",
      "\n",
      "Macro: 0.14685840832112715\n",
      "Micro: 0.5445328463979826\n",
      "Weighted: 0.49605431587571325\n",
      "Samples: 0.5323668029130675\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.6550449  0.60609388 0.42431762\n",
      " 0.         0.75722294 0.54137931 0.48145933 0.44047619 0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.20014144 0.31687106\n",
      " 0.2553802  0.67919799 0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.57175014 0.         0.\n",
      " 0.         0.         0.         0.         0.36732068 0.\n",
      " 0.         0.         0.         0.         0.38446849 0.\n",
      " 0.         0.         0.41176471 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.26580116 0.\n",
      " 0.         0.         0.         0.31697613 0.         0.\n",
      " 0.         0.         0.         0.         0.23154362 0.37596302\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.66246851 0.         0.         0.         0.\n",
      " 0.68       0.         0.45095168 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.30000000000000004\n",
      "\n",
      "Macro: 0.13750129601126673\n",
      "Micro: 0.5644489948234123\n",
      "Weighted: 0.4835268074596976\n",
      "Samples: 0.5529961494623258\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.6550449  0.60609388 0.42431762\n",
      " 0.         0.75722294 0.54137931 0.48145933 0.48150358 0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.         0.\n",
      " 0.2553802  0.67919799 0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.57175014 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.38446849 0.\n",
      " 0.         0.         0.41176471 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.26580116 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.23154362 0.37596302\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.66246851 0.         0.         0.         0.\n",
      " 0.68       0.         0.45095168 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.4\n",
      "\n",
      "Macro: 0.12432561456308483\n",
      "Micro: 0.5686488548809309\n",
      "Weighted: 0.45586010335718646\n",
      "Samples: 0.5577985167361759\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.6550449  0.60609388 0.42431762\n",
      " 0.         0.75722294 0.54137931 0.48145933 0.48150358 0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.         0.\n",
      " 0.2553802  0.67919799 0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.23154362 0.37596302\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.66246851 0.         0.         0.         0.\n",
      " 0.68       0.         0.45095168 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.5\n",
      "\n",
      "Macro: 0.11086355976707829\n",
      "Micro: 0.5661078808074625\n",
      "Weighted: 0.4350361697367789\n",
      "Samples: 0.5560125232298068\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.6550449  0.60609388 0.42431762\n",
      " 0.         0.75722294 0.54137931 0.         0.         0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.         0.\n",
      " 0.         0.67919799 0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.23154362 0.37596302\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.66246851 0.         0.         0.         0.\n",
      " 0.68       0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.6\n",
      "\n",
      "Macro: 0.094747997219651\n",
      "Micro: 0.5632044514196034\n",
      "Weighted: 0.4194355758616114\n",
      "Samples: 0.548452296447947\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.6550449  0.60609388 0.\n",
      " 0.         0.75722294 0.54137931 0.         0.         0.84465825\n",
      " 0.60435835 0.57574909 0.59107291 0.         0.         0.\n",
      " 0.         0.67919799 0.5382193  0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.37596302\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.7000000000000001\n",
      "\n",
      "Macro: 0.07651056516609162\n",
      "Micro: 0.5319649521337011\n",
      "Weighted: 0.3661163474468814\n",
      "Samples: 0.5248919290443675\n",
      "None:\n",
      " [0.90419806 0.83954097 0.81695389 0.6550449  0.         0.\n",
      " 0.         0.75722294 0.         0.         0.         0.84465825\n",
      " 0.60435835 0.         0.59107291 0.         0.         0.\n",
      " 0.         0.67919799 0.         0.60609388 0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.37596302\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.8\n",
      "\n",
      "Macro: 0.052968864092590474\n",
      "Micro: 0.4366530097699339\n",
      "Weighted: 0.274377447787197\n",
      "Samples: 0.4425029946288049\n",
      "None:\n",
      " [0.90419806 0.37442573 0.         0.         0.         0.\n",
      " 0.         0.75722294 0.         0.         0.         0.84465825\n",
      " 0.60435835 0.         0.59107291 0.         0.         0.\n",
      " 0.         0.67919799 0.         0.         0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "\n",
      "Calculated F-scores for cutoff: 0.9\n",
      "\n",
      "Macro: 0.045676944234262065\n",
      "Micro: 0.3859032861692\n",
      "Weighted: 0.22998915986674623\n",
      "Samples: 0.3936825820515091\n",
      "None:\n",
      " [0.         0.37442573 0.         0.         0.         0.\n",
      " 0.         0.75722294 0.         0.         0.         0.84465825\n",
      " 0.60435835 0.         0.59107291 0.         0.         0.\n",
      " 0.         0.67919799 0.         0.         0.83954097 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97346394 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "sequence = '''[**2177-6-24**] 11:37 AM  CT CHEST W/O CONTRAST                                           Clip # [**Clip Number (Radiology) 90497**]\r\n",
    " Reason: Parenchymal disease/mass s/p evacuation of fluid in vats\r\n",
    " Field of view: 43\r\n",
    " ______________________________________________________________________________\r\n",
    " [**Hospital 4**] MEDICAL CONDITION:\r\n",
    "   61 year old man with history of DOE X 5-6 weeks, with opacified hemithorax,\r\n",
    "  thoracentesis with elevated LDH s/p VATS\r\n",
    " REASON FOR THIS EXAMINATION:\r\n",
    "  Parenchymal disease/mass s/p evacuation of fluid in vats\r\n",
    " No contraindications for IV contrast\r\n",
    " ______________________________________________________________________________\r\n",
    "                                 FINAL REPORT\r\n",
    " INDICATION:  Dyspnea on exertion and pleural effusion of unknown etiology.\r\n",
    "\r\n",
    " TECHNIQUE:  Helical CT of the chest was performed at 5-mm collimation without\r\n",
    " intravenous contrast.\r\n",
    "\r\n",
    " COMPARISON:  CT chest of [**2177-6-22**].\r\n",
    "\r\n",
    " CT CHEST WITHOUT INTRAVENOUS CONTRAST:  In the interval since the prior study,\r\n",
    " three chest tubes have been placed in the right hemithorax.  Much of the right\r\n",
    " pleural effusion has been drained, with some residual locules seen\r\n",
    " peripherally.  The majority of the right lung has re-expanded, with some areas\r\n",
    " of linear atelectasis at the right base.  No focal lung mass is identified.\r\n",
    " The left lung is clear.  The airways are patent to the segmental bronchi\r\n",
    " bilaterally.  There is calcification of the coronary vessels.  The heart and\r\n",
    " great vessels are otherwise normal in appearance.  An enlarged right\r\n",
    " paratracheal lymph node is unchanged in appearance, measuring approximately 15\r\n",
    " mm.  There is probably an enlarged right hilar lymph node, although this is\r\n",
    " difficult to detect on this non-contrast study.\r\n",
    "\r\n",
    " In the imaged upper abdomen, a 14-mm lymph node anterior to the liver is again\r\n",
    " seen.  Surgical clips are seen in the right upper quadrant.  The superior\r\n",
    " portions of the pancreas, spleen, adrenal glands and kidneys are normal in\r\n",
    " appearance. Rounded thickening of the right costovertebral sulcus pleura\r\n",
    " extends into and past these inferior images.\r\n",
    "\r\n",
    " No destructive osseous lesions are identified.  There is minimal subcutaneous\r\n",
    " emphysema along the right chest.\r\n",
    "\r\n",
    " IMPRESSION:  Interval near-complete resolution of right pleural effusion, with\r\n",
    " some peripheral locules remaining.  No evidence of lung mass.  Several\r\n",
    " enlarged lymph nodes in the mediastinum persist.  An area of right posterior\r\n",
    " pleural thickening has an unusual rounded appearance and is incompletely\r\n",
    " evaluated on the current study; if there is clinical concern regarding this\r\n",
    " area, abdominal CT may be obtained.'''\r\n",
    "predictions, probs = model.predict([sequence])\r\n",
    "print(probs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.35s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.02it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.03378296 0.07354736 0.06561279 0.13659668 0.03396606 0.07196045\n",
      "  0.08587646 0.02174377 0.05490112 0.04995728 0.06744385 0.02006531\n",
      "  0.02964783 0.07366943 0.07720947 0.05041504 0.03253174 0.0526123\n",
      "  0.03649902 0.03179932 0.0030632  0.35253906 0.03277588 0.0350647\n",
      "  0.04077148 0.03283691 0.10778809 0.02400208 0.03314209 0.02641296\n",
      "  0.02246094 0.025177   0.16687012 0.01942444 0.02763367 0.03942871\n",
      "  0.16687012 0.02981567 0.0282135  0.03390503 0.10302734 0.02737427\n",
      "  0.06359863 0.0970459  0.06817627 0.04360962 0.08312988 0.20349121\n",
      "  0.02288818 0.0970459  0.0271759  0.02207947 0.02522278 0.02333069\n",
      "  0.0526123  0.15258789 0.01945496 0.04083252 0.12731934 0.02655029\n",
      "  0.02319336 0.00760651 0.00421524 0.00542831 0.00408554 0.00473785\n",
      "  0.00510025 0.00605011 0.00503922 0.00443268 0.00411606 0.00448608\n",
      "  0.00445175 0.00551224 0.00460815 0.00503922 0.00453949 0.04031372\n",
      "  0.01872253 0.04272461 0.016922   0.03555298 0.08416748 0.0274353\n",
      "  0.03079224 0.02059937 0.03378296 0.03533936 0.03067017 0.99511719\n",
      "  0.005867   0.02780151 0.00737381 0.00669098 0.00619125 0.00508118\n",
      "  0.03038025 0.02650452 0.00731659 0.03521729 0.02954102]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "with open(\"top_codes_orderbylabel.txt\", 'r') as file_handle:\r\n",
    "    top101ICD9_list = file_handle.readlines()\r\n",
    "\r\n",
    "print(len(top101ICD9_list))\r\n",
    "\r\n",
    "df_map = pd.read_csv(\"D_ICD_DIAGNOSES_WITH_ICD10CSV.csv\", sep=',')\r\n",
    "df_map.head()\r\n",
    "\r\n",
    "\r\n",
    "def return_topk_diagnosis_from_predictions_in_icd9(pred_tensor, topk_icd_list, topk): \r\n",
    "    #df_map is the mapping of all icd9, icd10 and short form and long form of diagnosis\r\n",
    "    #topk_icd9_list is the icd9 codes of top 100 diseases and \"Other\" in mimic 3\r\n",
    "    #pred_array is the array (tensor) of probabilities of diseases. shape (1,150)\r\n",
    "    #assert()\r\n",
    "    #if(type(pred_tensor) != 'numpy.ndarray'):\r\n",
    "    #    tmp = pred_tensor.numpy().reshape(-1)\r\n",
    "    #else:\r\n",
    "    tmp = pred_tensor.reshape(-1)\r\n",
    "    #print(type(tmp))\r\n",
    "    tmp = np.argpartition(tmp, -topk )[-topk:]#a.numpy() converts the tensor to numpy array\r\n",
    "    \r\n",
    "    topkICDPreds = []\r\n",
    "    for idx in tmp:\r\n",
    "        icd =  topk_icd_list[idx].strip()\r\n",
    "    #print(icd)\r\n",
    "        topkICDPreds.append(icd)\r\n",
    "    \r\n",
    "    return topkICDPreds   #return a List of topk  icd 9 codes\r\n",
    "    \r\n",
    "    # If the probability is same the icd9 method and text method may create erronous results. \r\n",
    "    # a better solutions is to return icd 9 and text description together.\r\n",
    "def return_topk_diagnosis_from_predictions_in_Text(pred_tensor,topk_icd_list, topk, df_map): \r\n",
    "    #df_map is the mapping of all icd9, icd10 and short form and long form of diagnosis\r\n",
    "    #topk_icd9_list is the icd9 codes of top 101 diseases in mimic 3\r\n",
    "    #pred_array is the array (tensor) of probabilities of diseases. shape (1,150)\r\n",
    "    topkICDPreds = return_topk_diagnosis_from_predictions_in_icd9(pred_tensor, topk_icd_list,topk)  \r\n",
    "    predictions = df_map.loc[df_map['ICD9_CODE'].isin(topkICDPreds)]\r\n",
    "    print(predictions)\r\n",
    "    #print(predictions)\r\n",
    "    text_preds = predictions['LONG_TITLE'].tolist()\r\n",
    "    if \"OTHER\" in topkICDPreds:\r\n",
    "        idx = topkICDPreds.index(\"OTHER\")\r\n",
    "        text_preds.insert(idx,\"OTHER\")\r\n",
    "    #text_preds = [s.strip() for s in text_preds]\r\n",
    "    return text_preds\r\n",
    "\r\n",
    "def return_topk_diagnosis_from_predictions_with_probability (pred_tensor,topk_icd_list, topk, df_map): \r\n",
    "    #df_map is the mapping of all icd9, icd10 and short form and long form of diagnosis\r\n",
    "    #topk_icd9_list is the icd9 codes of top 101 diseases in mimic 3\r\n",
    "    #pred_array is the array (tensor) of probabilities of diseases. shape (1,150)\r\n",
    "    topkICDPreds = return_topk_diagnosis_from_predictions_in_icd9(pred_tensor, topk_icd_list, topk)\r\n",
    "    text_preds = return_topk_diagnosis_from_predictions_in_Text(pred_tensor, topk_icd_list, topk, df_map)\r\n",
    "    \r\n",
    "    probs = pred_tensor.reshape(-1)\r\n",
    "    tmp = np.argpartition(probs, -topk )[-topk:]    #a.numpy() converts the tensor to numpy array\r\n",
    "    topkprobs = []  # to store the topk highest probabilities\r\n",
    "    #topkICDPreds = {}\r\n",
    "    for idx in tmp:\r\n",
    "        topkprobs.append(probs[idx])\r\n",
    "        \r\n",
    "    prob_dict = dict(zip(topkICDPreds,topkprobs))\r\n",
    "    text_dict = dict(zip(topkICDPreds,text_preds))\r\n",
    "    \r\n",
    "    if 'OTHER' in topkICDPreds:\r\n",
    "        text_dict['OTHER'] = \"Any Other Disease Not Managed by Current Version\"    \r\n",
    "    return prob_dict, text_dict\r\n",
    "\r\n",
    "def return_actual_diagnoses (y_true, topk_icd_list, df_map):\r\n",
    "    tmp = np.where(np.stack(seq2_actpred)==1)\r\n",
    "    tmp = list(tmp[0])  # reads the list containing all elements of tuple\r\n",
    "    topkICDPreds = []\r\n",
    "    for i in range(len(tmp)):\r\n",
    "        icd =  topk_icd_list[i].strip()\r\n",
    "        topkICDPreds.append(icd)\r\n",
    "    \r\n",
    "    predictions = df_map.loc[df_map['ICD9_CODE'].isin(topkICDPreds)]\r\n",
    "    text_preds = predictions['LONG_TITLE'].tolist()\r\n",
    "    \r\n",
    "    text_dict = dict(zip(topkICDPreds,text_preds))\r\n",
    "    return text_dict"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "101\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "def return_topk_diagnosis_from_predictions(pred_tensor,topk_icd_list,df_map,**kwargs):\r\n",
    "    # topk, return_prob, return_diagnosis       \r\n",
    "    if not 'topk' in kwargs.keys():\r\n",
    "        topk=5\r\n",
    "    else:\r\n",
    "        topk= kwargs['topk']\r\n",
    "\r\n",
    "    if 'return_prob' in kwargs.keys():\r\n",
    "        return_prob =  kwargs['return_prob']\r\n",
    "    else:\r\n",
    "        return_prob = False     \r\n",
    "        \r\n",
    "    if 'return_diagnosis' in kwargs.keys():\r\n",
    "        return_diagnosis =  kwargs['return_diagnosis']\r\n",
    "    else:\r\n",
    "        return_diagnosis = False\r\n",
    "              \r\n",
    "    #print(f\"Parameters: topk:{topk}  return_prob:{return_prob}  return_diagnosis:{return_diagnosis}\")\r\n",
    "\r\n",
    "    if ( not return_prob and  not return_diagnosis):\r\n",
    "        print(f\" 1 activated\")\r\n",
    "        tmp = pred_tensor.reshape(-1)\r\n",
    "        tmp = np.argpartition(tmp, -topk )[-topk:]#a.numpy() converts the tensor to numpy array\r\n",
    "        topkICDPreds = []\r\n",
    "        for idx in tmp:\r\n",
    "            icd =  topk_icd_list[idx].strip()\r\n",
    "    #print(icd)\r\n",
    "            topkICDPreds.append(icd)\r\n",
    "        return topkICDPreds\r\n",
    "        #return return_topk_diagnosis_from_predictions_in_icd9(pred_tensor, topk_icd_list, topk)\r\n",
    "    elif  (return_prob and not return_diagnosis):\r\n",
    "        print(f\" 2 activated\")\r\n",
    "       # topkICDPreds = return_topk_diagnosis_from_predictions_in_icd9(pred_tensor, topk_icd_list, topk)\r\n",
    "        prob = pred_tensor.reshape(-1)\r\n",
    "        indices = np.argpartition(prob, -topk )[-topk:]#a.numpy() converts the tensor to numpy array\r\n",
    "        #print(indices)\r\n",
    "        topkICDPreds = []\r\n",
    "        topkprobs = []  # to store the topk highest probabilities\r\n",
    "        for idx in indices:\r\n",
    "            icd =  topk_icd_list[idx].strip()\r\n",
    "            topkICDPreds.append(icd)\r\n",
    "            topkprobs.append(prob[idx])   \r\n",
    "        #print(topkICDPreds)\r\n",
    "        #print(topkprobs)\r\n",
    "        prob_dict = dict(zip(topkICDPreds,topkprobs))\r\n",
    "        return prob_dict\r\n",
    "    elif  ( not return_prob and return_diagnosis):\r\n",
    "        print(f\" 3 activated\")\r\n",
    "        prob = pred_tensor.reshape(-1)\r\n",
    "        indices = np.argpartition(prob, -topk )[-topk:]#a.numpy() converts the tensor to numpy array\r\n",
    "        topkICDPreds = []\r\n",
    "        for idx in indices:\r\n",
    "            icd =  topk_icd_list[idx].strip()\r\n",
    "            topkICDPreds.append(icd)\r\n",
    "        try:\r\n",
    "            predictions = df_map.loc[df_map['ICD9_CODE'].isin(topkICDPreds)]\r\n",
    "            text_preds = predictions['LONG_TITLE'].tolist()\r\n",
    "            if \"OTHER\" in topkICDPreds:\r\n",
    "                idx = topkICDPreds.index(\"OTHER\")\r\n",
    "                text_preds.insert(idx,\"OTHER\")\r\n",
    "            text_dict = dict(zip(topkICDPreds,text_preds))\r\n",
    "            return text_dict\r\n",
    "        except:\r\n",
    "            print (f\"exception occurred in diagnoses calculation.\")\r\n",
    "            return\r\n",
    "    elif (return_prob and return_diagnosis):\r\n",
    "        print(f\" 4 activated\")\r\n",
    "        prob = pred_tensor.reshape(-1)\r\n",
    "        indices = np.argpartition(prob, -topk )[-topk:]#a.numpy() converts the tensor to numpy array\r\n",
    "        topkICDPreds = []\r\n",
    "        topkprobs = []  # to store the topk highest probabilities\r\n",
    "        for idx in indices:\r\n",
    "            icd =  topk_icd_list[idx].strip()\r\n",
    "            topkICDPreds.append(icd)\r\n",
    "            topkprobs.append(prob[idx])\r\n",
    "        try:\r\n",
    "            predictions = df_map.loc[df_map['ICD9_CODE'].isin(topkICDPreds)]\r\n",
    "            text_preds = predictions['LONG_TITLE'].tolist()\r\n",
    "            if \"OTHER\" in topkICDPreds:\r\n",
    "                idx = topkICDPreds.index(\"OTHER\")\r\n",
    "                text_preds.insert(idx,\"OTHER\")\r\n",
    "            text_dict = dict(zip(topkICDPreds,text_preds))\r\n",
    "            prob_dict = dict(zip(topkICDPreds,topkprobs))\r\n",
    "            return prob_dict, text_dict\r\n",
    "        except:\r\n",
    "            print (f\"exception occurred in diagnoses calculation.\")\r\n",
    "            return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "tmp = np.where(np.stack(seq2_actpred)==1)\r\n",
    "#print(tmp)\r\n",
    "tmp = list(tmp[0])\r\n",
    "topkICDPreds = []\r\n",
    "for i in range(len(tmp)):\r\n",
    "    #print(tmp[i])\r\n",
    "    icd =  top101ICD9_list[i].strip()\r\n",
    "    topkICDPreds.append(icd)\r\n",
    "print(topkICDPreds)\r\n",
    "predictions = df_map.loc[df_map['ICD9_CODE'].isin(topkICDPreds)]\r\n",
    "print(predictions.head())"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "#print(return_topk_diagnosis_from_predictions_in_icd9(probs,top101ICD9_list,5))\r\n",
    "#print(return_topk_diagnosis_from_predictions_in_Text(probs,top101ICD9_list,5, df_map))\r\n",
    "#prob_dict,text_dict = return_topk_diagnosis_from_predictions(probs,top101ICD9_list,df_map, topk=7, return_prob=True, return_diagnosis=True)\r\n",
    "text_dict = return_topk_diagnosis_from_predictions(probs,top101ICD9_list,df_map, topk=7, return_prob=False, return_diagnosis=False)\r\n",
    "#print(prob_dict)\r\n",
    "print(text_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 1 activated\n",
      "['25000', '5849', '4280', '42731', '51881', '4019', 'OTHER']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "prob_dict, text_dict = return_topk_diagnosis_from_predictions_with_probability(probs,top101ICD9_list,5, df_map)\r\n",
    "for key in prob_dict.keys():\r\n",
    "    print(f\"ICD9_code: {key}  Diagnoses:{text_dict[key]}  Probability:{prob_dict[key]}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ICD9_code: 4280  Diagnoses:Acute respiratory failure  Probability:0.1668701171875\n",
      "ICD9_code: 42731  Diagnoses:Congestive heart failure, unspecified  Probability:0.1668701171875\n",
      "ICD9_code: 51881  Diagnoses:Atrial fibrillation  Probability:0.2034912109375\n",
      "ICD9_code: 4019  Diagnoses:Unspecified essential hypertension  Probability:0.3525390625\n",
      "ICD9_code: OTHER  Diagnoses:Any Other Disease Not Managed by Current Version  Probability:0.9951171875\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\r\n",
    "sequence2 =''' \"Nursing Progress Note 0700-1900\r\n",
    "\r\n",
    "Events:  Given 1.5 mg ativan prior to MRI, unable to obtain good scan D/T anxiety & agitation.  Repeat NA at noon 119\r\n",
    "\r\n",
    "Neuro:  A&Ox2, unsure of date.  MAE, PERRLA.  Remains on CIWA scale scores less than 5, but remains agitated at times. Given 2.5 mg PO valium at noon with little effect.  Remains in C-collar until MRI can clear her C-spine.\r\n",
    "\r\n",
    "Resp:  Lung sounds clear diminished at bases, RR mid teens sats 95-97 on RA.  Respirations even and unlabored.  + non-productive cough.\r\n",
    "\r\n",
    "Cardiac:  Tele SR 70's without ectopy.  Hemodynamically stable.  + 3 pt/dp, no edema\r\n",
    "\r\n",
    "GI:  PO intake very poor, refusing meals, drinking ensures TID.  + BS in 4 quadrents, abdomen soft non-distended,  No BM since admission on bowel regimen\r\n",
    "\r\n",
    "Renal:  Foley D/C'd at noon DTV.  UOP has been marginal.  Urine dark and amber in color\r\n",
    "\r\n",
    "ID:  afebrile on Flagyl/ceftriaxone with PNA.\r\n",
    "\r\n",
    "Skin:  Intact no current issues\r\n",
    "\r\n",
    "FEN:  NA at noon 119 remains on 1.5 liter fluid restriction.  repeat NA due at [**2162**]\r\n",
    "\r\n",
    "Plan:\r\n",
    "\r\n",
    "1.  C/O to floor\r\n",
    "2.  Monitor neuro status, C-collar repeat MRI in AM\r\n",
    "3.  Serial NA checks fluid restrict\r\n",
    "4.  IV antibiotics as ordered\r\n",
    "5.  Routine monitoring and care'''\r\n",
    "predictions, probs2 = model.predict([sequence2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.08s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.63it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['4280', 'OTHER', '42731', '4019', '51881']\n",
      "['Acute respiratory failure', 'Congestive heart failure, unspecified', 'Atrial fibrillation', 'Unspecified essential hypertension']\n",
      "ICD9_code: 4280  Diagnoses:Acute respiratory failure  Probability:0.10577392578125\n",
      "ICD9_code: OTHER  Diagnoses:Any Other Disease Not Managed by Current Version  Probability:0.99560546875\n",
      "ICD9_code: 42731  Diagnoses:Atrial fibrillation  Probability:0.11260986328125\n",
      "ICD9_code: 4019  Diagnoses:Unspecified essential hypertension  Probability:0.32177734375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'51881'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ac2cfc26fcd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_topk_diagnosis_from_predictions_in_Text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtop101ICD9_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprob_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"ICD9_code: {key}  Diagnoses:{text_dict[key]}  Probability:{prob_dict[key]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '51881'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prob_dict, text_dict = return_topk_diagnosis_from_predictions_with_probability(probs2,top101ICD9_list,5, df_map)\r\n",
    "print(return_topk_diagnosis_from_predictions_in_icd9(probs2,top101ICD9_list,5))\r\n",
    "print(return_topk_diagnosis_from_predictions_in_Text(probs2,top101ICD9_list,5,df_map))\r\n",
    "for key in prob_dict.keys():\r\n",
    "    print(f\"ICD9_code: {key}  Diagnoses:{text_dict[key]}  Probability:{prob_dict[key]}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "seq2_actpred= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \r\n",
    "               0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \r\n",
    "               0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n",
    "text_dict = return_actual_diagnoses (seq2_actpred, top101ICD9_list,df_map)\r\n",
    "for key in text_dict.keys():\r\n",
    "    print(f\"ICD9_code: {key}  Diagnoses:{text_dict[key]}\")\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ICD9_code: 01193  Diagnoses:Other primary cardiomyopathies\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('py38tfpt': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "0e7c72472f5f68e485bf0eebb79f8beaa3d8097226cc86dd381c1b0150ce4bc6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}